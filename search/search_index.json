{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"video_features allows you to extract features from video clips. It supports a variety of extractors and modalities, i. e. visual appearance, optical flow, and audio. Supported Models Action Recognition S3D (Kinetics 400) R(2+1)d RGB (IG-65M, Kinetics 400) I3D-Net RGB + Flow (Kinetics 400) Sound Recognition VGGish (AudioSet) Optical Flow RAFT (FlyingChairs, FlyingThings3D, Sintel, KITTI) PWC-Net (Sintel) Frame-wise Features CLIP ResNet-18,34,50,101,152 (ImageNet) Quick Start # clone the repo and change the working directory git clone https://github.com/v-iashin/video_features.git cd video_features # install environment conda env create -f conda_env_torch_zoo.yml # load the environment conda activate torch_zoo # extract r(2+1)d features for the sample videos python main.py \\ feature_type=r21d \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" # if you have many GPUs, just run this command from another terminal with another device # device can also be \"cpu\" If you are more comfortable with Docker, there is a Docker image with a pre-installed environment that supports all models. Check out the Docker support . documentation page. Multi-GPU and Multi-Node Setups With video_features , it is easy to parallelize feature extraction among many GPUs. It is enough to start the script in another terminal with another GPU (or even the same one) pointing to the same output folder and input video paths. The script will check if the features already exist and skip them. It will also try to load the feature file to check if it is corrupted (i.e. not openable). This approach allows you to continue feature extraction if the previous script failed for some reason. If you have an access to a GPU cluster with shared disk space you may scale extraction with as many GPUs as you can by creating several single-GPU jobs with the same command. Since each time the script is run the list of input files is shuffled, you don't need to worry that workers will be processing the same video. On a rare occasion when the collision happens, the script will rewrite previously extracted features. Used in SpecVQGAN branch specvqgan BMT branch bmt MDVC branch mdvc Please, let me know if you found this repo useful for your projects or papers. Acknowledgements @Kamino666 : added CLIP model as well as Windows and CPU support (and many other small things). @borijang : for solving bugs with file names, I3D checkpoint loading enhancement and code style improvements. @ohjho : added support of 37-layer R(2+1)d favors.","title":"Home"},{"location":"#supported-models","text":"Action Recognition S3D (Kinetics 400) R(2+1)d RGB (IG-65M, Kinetics 400) I3D-Net RGB + Flow (Kinetics 400) Sound Recognition VGGish (AudioSet) Optical Flow RAFT (FlyingChairs, FlyingThings3D, Sintel, KITTI) PWC-Net (Sintel) Frame-wise Features CLIP ResNet-18,34,50,101,152 (ImageNet)","title":"Supported Models"},{"location":"#quick-start","text":"# clone the repo and change the working directory git clone https://github.com/v-iashin/video_features.git cd video_features # install environment conda env create -f conda_env_torch_zoo.yml # load the environment conda activate torch_zoo # extract r(2+1)d features for the sample videos python main.py \\ feature_type=r21d \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" # if you have many GPUs, just run this command from another terminal with another device # device can also be \"cpu\" If you are more comfortable with Docker, there is a Docker image with a pre-installed environment that supports all models. Check out the Docker support . documentation page.","title":"Quick Start"},{"location":"#multi-gpu-and-multi-node-setups","text":"With video_features , it is easy to parallelize feature extraction among many GPUs. It is enough to start the script in another terminal with another GPU (or even the same one) pointing to the same output folder and input video paths. The script will check if the features already exist and skip them. It will also try to load the feature file to check if it is corrupted (i.e. not openable). This approach allows you to continue feature extraction if the previous script failed for some reason. If you have an access to a GPU cluster with shared disk space you may scale extraction with as many GPUs as you can by creating several single-GPU jobs with the same command. Since each time the script is run the list of input files is shuffled, you don't need to worry that workers will be processing the same video. On a rare occasion when the collision happens, the script will rewrite previously extracted features.","title":"Multi-GPU and Multi-Node Setups"},{"location":"#used-in","text":"SpecVQGAN branch specvqgan BMT branch bmt MDVC branch mdvc Please, let me know if you found this repo useful for your projects or papers.","title":"Used in"},{"location":"#acknowledgements","text":"@Kamino666 : added CLIP model as well as Windows and CPU support (and many other small things). @borijang : for solving bugs with file names, I3D checkpoint loading enhancement and code style improvements. @ohjho : added support of 37-layer R(2+1)d favors.","title":"Acknowledgements"},{"location":"meta/docker/","text":"Docker support For your convenience, there is also a Docker image with the pre-installed environments that supports all models. The Docker image does not have the video_features library inside which allows you to tweak the code locally, mount the new version, and just use the container as an environment. It is assumed that you have Docker and nvidia-container-runtime installed. Setup Start by cloning the repo locally if you haven't done it already git clone https://github.com/v-iashin/video_features.git Download the docker image or build it yourself: docker pull iashin/video_features # preventing newer versions of the image to be downloaded unexpectedly docker tag iashin/video_features video_features # or # docker build - < ./video_features/Dockerfile --tag video_features Once it is done, mount ( --mount ) the cloned repository folder, and initialize a container with the 0th GPU but remember: just like with any mount, a change from inside of the container will be reflected in the mounted folder ( /absolute/path/to/video_features/ ). : docker run -it \\ --mount type=bind,source=\"/absolute/path/to/video_features/\",destination=\"/home/ubuntu/video_features/\" \\ --shm-size 8G \\ -it --gpus '\"device=0\"' \\ video_features:latest \\ bash # and you should get the bash shell: # ubuntu@56b1bf77a20c:~$ Check if a GPU is available to PyTorch: # ubuntu@56b1bf77a20c:~$ python -c \"import torch; print(torch.cuda.is_available())\" # True Finally, try to extract video features: # cd to `./video_features` # ubuntu@56b1bf77a20c:~/video_features $ python main.py \\ feature_type=r21d \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Extract features from custom videos You need to mount the folders with video files before you start the container. If the folder with custom videos is already in ./video_features , you don't have to do anything. Any changes from inside of the container will be reflected in your original dataset (use a backup!). Here is an example of mounting a folder from somewhere else (mounts /absolute/path/somewhere/else/ to /home/ubuntu/video_features/dataset ): docker run -it \\ --mount type=bind,source=\"/absolute/path/to/video_features/\",destination=\"/home/ubuntu/video_features/\" \\ --mount type=bind,source=\"/absolute/path/somewhere/else/\",destination=\"/home/ubuntu/video_features/dataset/\" \\ --shm-size 8G \\ -it --gpus '\"device=0\"' \\ video_features:latest \\ bash # ubuntu@56b1bf77a20c:~$ ls ./video_features # ... dataset ... If you want to save outputs to another folder on your local machine, you may want to mount it as well: e.g. by adding ... --mount type=bind,source=\"/absolute/path/somewhere/else/\",destination=\"/home/ubuntu/video_features/output/\" \\ ... Then, run your command. For instance: # cd to `./video_features` # ubuntu@56b1bf77a20c:~/video_features $ python main.py \\ feature_type=r21d \\ device=\"cuda:0\" \\ video_paths=\"[./dataset/vid_1.mp4, ./dataset/vid_2.mp4]\" \\ on_extraction=\"save_numpy\" # you should have features in `./output` # (and in the source location if you mount to it) Switching conda environments By default, the torch_zoo environment is activated once you attach the shell. The image supports both conda environments and you can switch it simply as follows: # ubuntu@56b1bf77a20c:~$ conda activate pwc conda deactivate conda activate torch_zoo # which python","title":"Docker support"},{"location":"meta/docker/#docker-support","text":"For your convenience, there is also a Docker image with the pre-installed environments that supports all models. The Docker image does not have the video_features library inside which allows you to tweak the code locally, mount the new version, and just use the container as an environment. It is assumed that you have Docker and nvidia-container-runtime installed.","title":"Docker support"},{"location":"meta/docker/#setup","text":"Start by cloning the repo locally if you haven't done it already git clone https://github.com/v-iashin/video_features.git Download the docker image or build it yourself: docker pull iashin/video_features # preventing newer versions of the image to be downloaded unexpectedly docker tag iashin/video_features video_features # or # docker build - < ./video_features/Dockerfile --tag video_features Once it is done, mount ( --mount ) the cloned repository folder, and initialize a container with the 0th GPU but remember: just like with any mount, a change from inside of the container will be reflected in the mounted folder ( /absolute/path/to/video_features/ ). : docker run -it \\ --mount type=bind,source=\"/absolute/path/to/video_features/\",destination=\"/home/ubuntu/video_features/\" \\ --shm-size 8G \\ -it --gpus '\"device=0\"' \\ video_features:latest \\ bash # and you should get the bash shell: # ubuntu@56b1bf77a20c:~$ Check if a GPU is available to PyTorch: # ubuntu@56b1bf77a20c:~$ python -c \"import torch; print(torch.cuda.is_available())\" # True Finally, try to extract video features: # cd to `./video_features` # ubuntu@56b1bf77a20c:~/video_features $ python main.py \\ feature_type=r21d \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"","title":"Setup"},{"location":"meta/docker/#extract-features-from-custom-videos","text":"You need to mount the folders with video files before you start the container. If the folder with custom videos is already in ./video_features , you don't have to do anything. Any changes from inside of the container will be reflected in your original dataset (use a backup!). Here is an example of mounting a folder from somewhere else (mounts /absolute/path/somewhere/else/ to /home/ubuntu/video_features/dataset ): docker run -it \\ --mount type=bind,source=\"/absolute/path/to/video_features/\",destination=\"/home/ubuntu/video_features/\" \\ --mount type=bind,source=\"/absolute/path/somewhere/else/\",destination=\"/home/ubuntu/video_features/dataset/\" \\ --shm-size 8G \\ -it --gpus '\"device=0\"' \\ video_features:latest \\ bash # ubuntu@56b1bf77a20c:~$ ls ./video_features # ... dataset ... If you want to save outputs to another folder on your local machine, you may want to mount it as well: e.g. by adding ... --mount type=bind,source=\"/absolute/path/somewhere/else/\",destination=\"/home/ubuntu/video_features/output/\" \\ ... Then, run your command. For instance: # cd to `./video_features` # ubuntu@56b1bf77a20c:~/video_features $ python main.py \\ feature_type=r21d \\ device=\"cuda:0\" \\ video_paths=\"[./dataset/vid_1.mp4, ./dataset/vid_2.mp4]\" \\ on_extraction=\"save_numpy\" # you should have features in `./output` # (and in the source location if you mount to it)","title":"Extract features from custom videos"},{"location":"meta/docker/#switching-conda-environments","text":"By default, the torch_zoo environment is activated once you attach the shell. The image supports both conda environments and you can switch it simply as follows: # ubuntu@56b1bf77a20c:~$ conda activate pwc conda deactivate conda activate torch_zoo # which python","title":"Switching conda environments"},{"location":"models/clip/","text":"CLIP The CLIP features are extracted at each frame of the provided video. CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. We use CLIP's official augmentations and extract vision features from its image encoder. The implementation uses the OpenAI CLIP . The extracted features are going to be of size num_frames x 512 . We additionally output timesteps in ms for each feature and fps of the video. Set up the Environment for CLIP Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml Quick Start Activate the environment conda activate torch_zoo and extract features at 1 fps from ./sample/v_GGSY1Qvo990.mp4 video and output results. python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ extraction_fps=1 \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ on_extraction=\"print\" Supported Arguments Argument Default Description model_name \"ViT-B/32\" A variant of CLIP. \"ViT-B/16\" , \"RN50x16\" , \"RN50x4\" , \"RN101\" , \"RN50\" , and \"custom\" are supported. batch_size 1 You may speed up extraction of features by increasing the batch size as much as your GPU permits. extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will print the predictions of the model on a down-stream task. It is useful for debugging. pred_texts null If show_pred=true , the texts specified in pred_texts are used for zero-shot classification (e.g. pred_texts=\"['a dog smiles', 'a woman is lifting']\" ). If pred_texts is unspecified, Kinetics 400 classes will be used. Examples Start by activating the environment conda activate torch_zoo It is pretty much the same procedure as with other features. Here we take ViT/B-32 as an example, but we also support ViT-B/16 , RN50x16 , RN50x4 , RN101 , RN50 and others in OpenAI CLIP implementation . In addition, if you want to use your weights, you need to copy your weights to models/clip/checkpoints , rename it CLIP-custom.pth and specify model_name=custom . python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to save the features, use --on_extraction save_numpy (or save_pickle ) \u2013 by default, the features are saved in ./output/ or where --output_path specifies. In the case of frame-wise features, besides features, it also saves timestamps in ms and the original fps of the video into the same folder with features. python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ device=\"cuda:0\" \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt We may increase the extraction speed with batching. Therefore, frame-wise features have --batch_size argument, which defaults to 1 . python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ device=\"cuda:0\" \\ batch_size=128 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to extract features at a certain fps, add --extraction_fps argument python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ device=\"cuda:0\" \\ extraction_fps=5 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to verify the extracted features, you can set show_pred=\"true\" and provide several sentences with pred_texts argument. The value of pred_texts should be a list of strings. The probability that each frame corresponds to all the sentences you provide will be output. python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ device=\"cuda:0\" \\ extraction_fps=1 \\ show_pred=\"true\" \\ pred_texts=\"['a dog smiles', 'a woman is lifting']\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" You will get the output for each frame like: Logits | Prob. | Label 23.061 | 0.962 | a dog smiles 19.824 | 0.038 | a woman is lifting Logits | Prob. | Label 22.770 | 0.963 | a dog smiles 19.520 | 0.037 | a woman is lifting Logits | Prob. | Label 24.619 | 0.929 | a dog smiles 22.048 | 0.071 | a woman is lifting ... Logits | Prob. | Label 30.966 | 1.000 | a woman is lifting 15.272 | 0.000 | a dog smiles Logits | Prob. | Label 32.671 | 1.000 | a woman is lifting 15.413 | 0.000 | a dog smiles Logits | Prob. | Label 32.555 | 1.000 | a woman is lifting 16.151 | 0.000 | a dog smiles ... You may also leave pred_texts unspecified or null (None) if you wish to apply CLIP for zero-shot prediction on Kinetics 400. Credits The OpenAI CLIP implementation . The CLIP paper Thanks to @Kamino666 who adapted this model for video_features License The OpenAI CLIP implementation code is under MIT.","title":"CLIP"},{"location":"models/clip/#clip","text":"The CLIP features are extracted at each frame of the provided video. CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. We use CLIP's official augmentations and extract vision features from its image encoder. The implementation uses the OpenAI CLIP . The extracted features are going to be of size num_frames x 512 . We additionally output timesteps in ms for each feature and fps of the video.","title":"CLIP"},{"location":"models/clip/#set-up-the-environment-for-clip","text":"Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for CLIP"},{"location":"models/clip/#quick-start","text":"Activate the environment conda activate torch_zoo and extract features at 1 fps from ./sample/v_GGSY1Qvo990.mp4 video and output results. python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ extraction_fps=1 \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ on_extraction=\"print\"","title":"Quick Start"},{"location":"models/clip/#supported-arguments","text":"Argument Default Description model_name \"ViT-B/32\" A variant of CLIP. \"ViT-B/16\" , \"RN50x16\" , \"RN50x4\" , \"RN101\" , \"RN50\" , and \"custom\" are supported. batch_size 1 You may speed up extraction of features by increasing the batch size as much as your GPU permits. extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will print the predictions of the model on a down-stream task. It is useful for debugging. pred_texts null If show_pred=true , the texts specified in pred_texts are used for zero-shot classification (e.g. pred_texts=\"['a dog smiles', 'a woman is lifting']\" ). If pred_texts is unspecified, Kinetics 400 classes will be used.","title":"Supported Arguments"},{"location":"models/clip/#examples","text":"Start by activating the environment conda activate torch_zoo It is pretty much the same procedure as with other features. Here we take ViT/B-32 as an example, but we also support ViT-B/16 , RN50x16 , RN50x4 , RN101 , RN50 and others in OpenAI CLIP implementation . In addition, if you want to use your weights, you need to copy your weights to models/clip/checkpoints , rename it CLIP-custom.pth and specify model_name=custom . python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to save the features, use --on_extraction save_numpy (or save_pickle ) \u2013 by default, the features are saved in ./output/ or where --output_path specifies. In the case of frame-wise features, besides features, it also saves timestamps in ms and the original fps of the video into the same folder with features. python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ device=\"cuda:0\" \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt We may increase the extraction speed with batching. Therefore, frame-wise features have --batch_size argument, which defaults to 1 . python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ device=\"cuda:0\" \\ batch_size=128 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to extract features at a certain fps, add --extraction_fps argument python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ device=\"cuda:0\" \\ extraction_fps=5 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to verify the extracted features, you can set show_pred=\"true\" and provide several sentences with pred_texts argument. The value of pred_texts should be a list of strings. The probability that each frame corresponds to all the sentences you provide will be output. python main.py \\ feature_type=\"clip\" \\ model_name=\"ViT-B/32\" \\ device=\"cuda:0\" \\ extraction_fps=1 \\ show_pred=\"true\" \\ pred_texts=\"['a dog smiles', 'a woman is lifting']\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" You will get the output for each frame like: Logits | Prob. | Label 23.061 | 0.962 | a dog smiles 19.824 | 0.038 | a woman is lifting Logits | Prob. | Label 22.770 | 0.963 | a dog smiles 19.520 | 0.037 | a woman is lifting Logits | Prob. | Label 24.619 | 0.929 | a dog smiles 22.048 | 0.071 | a woman is lifting ... Logits | Prob. | Label 30.966 | 1.000 | a woman is lifting 15.272 | 0.000 | a dog smiles Logits | Prob. | Label 32.671 | 1.000 | a woman is lifting 15.413 | 0.000 | a dog smiles Logits | Prob. | Label 32.555 | 1.000 | a woman is lifting 16.151 | 0.000 | a dog smiles ... You may also leave pred_texts unspecified or null (None) if you wish to apply CLIP for zero-shot prediction on Kinetics 400.","title":"Examples"},{"location":"models/clip/#credits","text":"The OpenAI CLIP implementation . The CLIP paper Thanks to @Kamino666 who adapted this model for video_features","title":"Credits"},{"location":"models/clip/#license","text":"The OpenAI CLIP implementation code is under MIT.","title":"License"},{"location":"models/i3d/","text":"I3D (RGB + Flow) The Inflated 3D ( I3D ) features are extracted using a pre-trained model on Kinetics 400 . Here, the features are extracted from the second-to-the-last layer of I3D, before summing them up. Therefore, it outputs two tensors with 1024-d features: for RGB and flow streams. By default, it expects to input 64 RGB and flow frames ( 224x224 ) which spans 2.56 seconds of the video recorded at 25 fps. In the default case, the features will be of size Tv x 1024 where Tv = duration / 2.56 . Please note, this implementation uses either PWC-Net (the default) and RAFT optical flow extraction instead of the TV-L1 algorithm, which was used in the original I3D paper as it hampers speed. Yet, it might possibly lead to worse peformance. Our tests show that the performance is reasonable. You may test it yourself by providing --show_pred flag. CUDA 11 and GPUs like RTX 3090 and newer PWC optical flow back-end is not supported on CUDA 11 and, therefore, GPUs like RTX 3090 and newer. RGB-only model should still work. For details please check this issue #13 If you were able to fix it, please share your workaround. Feel free to use flow_type=raft RAFT during extraction. The PWC-Net does NOT support using CPU currently The PWC-Net uses cupy module, which makes it difficult to turn to a version that does not use the GPU. However, if you have solution, you may submit a PR. Supported Arguments Argument Default Description stack_size 64 The number of frames from which to extract features (or window size). step_size 64 The number of frames to step before extracting the next features. streams null I3D is a two-stream network. By default ( null or omitted) both RGB and flow streams are used. To use RGB- or flow-only models use rgb or flow . flow_type pwc By default, the flow-features of I3D will be calculated using optical from calculated with PWCNet (originally with TV-L1). Another supported model is raft . extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will print the predictions of the model on a down-stream task. It is useful for debugging. Set up the Environment for I3D Depending on whether you would like to use PWC-Net or RAFT for optical flow extraction, you will need to install separate conda environments \u2013 conda_env_pwc.yml and conda_env_torch_zoo , respectively # it will create a new conda environment called 'pwc' on your machine conda env create -f conda_env_pwc.yml # or/and if you would like to extract optical flow with RAFT conda env create -f conda_env_torch_zoo.yml Quick Start Activate the environment conda activate pwc if you would like to use RAFT as optical flow extractor use torch_zoo instead of pwc : and extract features from ./sample/v_ZNVhz7ctTq0.mp4 video and show the predicted classes python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4]\" \\ show_pred=true Examples Activate the environment conda activate pwc The following will extract I3D features for sample videos. The features are going to be extracted with the default parameters. python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" The video paths can be specified as a .txt file with paths python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ file_with_video_paths=./sample/sample_video_paths.txt It is also possible to extract features from either rgb or flow modalities individually ( --streams ) and, therefore, increasing the speed python main.py \\ feature_type=i3d \\ streams=flow \\ device=\"cuda:0\" \\ file_with_video_paths=./sample/sample_video_paths.txt To extract optical flow frames using RAFT approach, specify --flow_type raft . Note that using RAFT will make the extraction slower than with PWC-Net yet visual inspection of extracted flow frames suggests that RAFT has a better quality of the estimated flow # make sure to activate the correct environment (`torch_zoo`) # conda activate torch_zoo python main.py \\ feature_type=i3d \\ flow_type=raft \\ device=\"cuda:0\" \\ file_with_video_paths=./sample/sample_video_paths.txt The features can be saved as numpy arrays by specifying --on_extraction save_numpy or save_pickle . By default, it will create a folder ./output and will store features there python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt You can change the output folder using --output_path argument. Also, you may want to try to change I3D window and step sizes python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ stack_size=24 \\ step_size=24 \\ file_with_video_paths=./sample/sample_video_paths.txt By default, the frames are extracted according to the original fps of a video. If you would like to extract frames at a certain fps, specify --extraction_fps argument. python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ extraction_fps=25 \\ stack_size=24 \\ step_size=24 \\ file_with_video_paths=./sample/sample_video_paths.txt A fun note, the time span of the I3D features in the last example will match the time span of VGGish features with default parameters (24/25 = 0.96). If --keep_tmp_files is specified, it keeps them in --tmp_path which is ./tmp by default. Be careful with the --keep_tmp_files argument when playing with --extraction_fps as it may mess up the frames you extracted before in the same folder. Credits An implementation of PWC-Net in PyTorch The Official RAFT implementation (esp. ./demo.py ) . A port of I3D weights from TensorFlow to PyTorch The I3D paper: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset . License The wrapping code is MIT and the port of I3D weights from TensorFlow to PyTorch. However, PWC Net (default flow extractor) has GPL-3.0 and RAFT BSD 3-Clause .","title":"I3D (RGB + Flow)"},{"location":"models/i3d/#i3d-rgb-flow","text":"The Inflated 3D ( I3D ) features are extracted using a pre-trained model on Kinetics 400 . Here, the features are extracted from the second-to-the-last layer of I3D, before summing them up. Therefore, it outputs two tensors with 1024-d features: for RGB and flow streams. By default, it expects to input 64 RGB and flow frames ( 224x224 ) which spans 2.56 seconds of the video recorded at 25 fps. In the default case, the features will be of size Tv x 1024 where Tv = duration / 2.56 . Please note, this implementation uses either PWC-Net (the default) and RAFT optical flow extraction instead of the TV-L1 algorithm, which was used in the original I3D paper as it hampers speed. Yet, it might possibly lead to worse peformance. Our tests show that the performance is reasonable. You may test it yourself by providing --show_pred flag. CUDA 11 and GPUs like RTX 3090 and newer PWC optical flow back-end is not supported on CUDA 11 and, therefore, GPUs like RTX 3090 and newer. RGB-only model should still work. For details please check this issue #13 If you were able to fix it, please share your workaround. Feel free to use flow_type=raft RAFT during extraction. The PWC-Net does NOT support using CPU currently The PWC-Net uses cupy module, which makes it difficult to turn to a version that does not use the GPU. However, if you have solution, you may submit a PR.","title":"I3D (RGB + Flow)"},{"location":"models/i3d/#supported-arguments","text":"Argument Default Description stack_size 64 The number of frames from which to extract features (or window size). step_size 64 The number of frames to step before extracting the next features. streams null I3D is a two-stream network. By default ( null or omitted) both RGB and flow streams are used. To use RGB- or flow-only models use rgb or flow . flow_type pwc By default, the flow-features of I3D will be calculated using optical from calculated with PWCNet (originally with TV-L1). Another supported model is raft . extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will print the predictions of the model on a down-stream task. It is useful for debugging.","title":"Supported Arguments"},{"location":"models/i3d/#set-up-the-environment-for-i3d","text":"Depending on whether you would like to use PWC-Net or RAFT for optical flow extraction, you will need to install separate conda environments \u2013 conda_env_pwc.yml and conda_env_torch_zoo , respectively # it will create a new conda environment called 'pwc' on your machine conda env create -f conda_env_pwc.yml # or/and if you would like to extract optical flow with RAFT conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for I3D"},{"location":"models/i3d/#quick-start","text":"Activate the environment conda activate pwc if you would like to use RAFT as optical flow extractor use torch_zoo instead of pwc : and extract features from ./sample/v_ZNVhz7ctTq0.mp4 video and show the predicted classes python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4]\" \\ show_pred=true","title":"Quick Start"},{"location":"models/i3d/#examples","text":"Activate the environment conda activate pwc The following will extract I3D features for sample videos. The features are going to be extracted with the default parameters. python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" The video paths can be specified as a .txt file with paths python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ file_with_video_paths=./sample/sample_video_paths.txt It is also possible to extract features from either rgb or flow modalities individually ( --streams ) and, therefore, increasing the speed python main.py \\ feature_type=i3d \\ streams=flow \\ device=\"cuda:0\" \\ file_with_video_paths=./sample/sample_video_paths.txt To extract optical flow frames using RAFT approach, specify --flow_type raft . Note that using RAFT will make the extraction slower than with PWC-Net yet visual inspection of extracted flow frames suggests that RAFT has a better quality of the estimated flow # make sure to activate the correct environment (`torch_zoo`) # conda activate torch_zoo python main.py \\ feature_type=i3d \\ flow_type=raft \\ device=\"cuda:0\" \\ file_with_video_paths=./sample/sample_video_paths.txt The features can be saved as numpy arrays by specifying --on_extraction save_numpy or save_pickle . By default, it will create a folder ./output and will store features there python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt You can change the output folder using --output_path argument. Also, you may want to try to change I3D window and step sizes python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ stack_size=24 \\ step_size=24 \\ file_with_video_paths=./sample/sample_video_paths.txt By default, the frames are extracted according to the original fps of a video. If you would like to extract frames at a certain fps, specify --extraction_fps argument. python main.py \\ feature_type=i3d \\ device=\"cuda:0\" \\ extraction_fps=25 \\ stack_size=24 \\ step_size=24 \\ file_with_video_paths=./sample/sample_video_paths.txt A fun note, the time span of the I3D features in the last example will match the time span of VGGish features with default parameters (24/25 = 0.96). If --keep_tmp_files is specified, it keeps them in --tmp_path which is ./tmp by default. Be careful with the --keep_tmp_files argument when playing with --extraction_fps as it may mess up the frames you extracted before in the same folder.","title":"Examples"},{"location":"models/i3d/#credits","text":"An implementation of PWC-Net in PyTorch The Official RAFT implementation (esp. ./demo.py ) . A port of I3D weights from TensorFlow to PyTorch The I3D paper: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset .","title":"Credits"},{"location":"models/i3d/#license","text":"The wrapping code is MIT and the port of I3D weights from TensorFlow to PyTorch. However, PWC Net (default flow extractor) has GPL-3.0 and RAFT BSD 3-Clause .","title":"License"},{"location":"models/pwc/","text":"PWC-Net PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume frames are extracted for every consecutive pair of frames in a video. PWC-Net is pre-trained on Sintel Flow dataset . The implementation follows sniklaus/pytorch-pwc@f61389005 . CUDA 11 and GPUs like RTX 3090 and newer The current environment does not support CUDA 11 and, therefore, GPUs like RTX 3090 and newer. For details please check this issue #13 If you were able to fix it, please share your workaround. If you need an optical flow extractor, you are recommended to use RAFT . The PWC-Net does NOT support using CPU currently The PWC-Net uses cupy module, which makes it difficult to turn to a version that does not use the GPU. However, if you have solution, you may submit a PR. Set up the Environment for PWC Setup conda environment. # it will create a new conda environment called 'pwc' on your machine conda env create -f conda_env_pwc.yml Quick Start Activate the environment conda activate pwc and extract optical flow from ./sample/v_GGSY1Qvo990.mp4 and show the flow for each frame python main.py \\ feature_type=pwc \\ device=\"cuda:0\" \\ show_pred=true \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" Note , if show_pred=true , the window with predictions will appear, use any key to show the next frame. To use show_pred=true , a screen must be attached to the machine or X11 forwarding is enabled. Supported Arguments Argument Default Description batch_size 1 You may speed up extraction of features by increasing the batch size as much as your GPU permits. extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. side_size null If resized to the smaller edge ( resize_to_smaller_edge=true ), then min(W, H) = side_size , if to the larger: max(W, H), if null (None) no resize is performed. resize_to_smaller_edge true If false , the larger edge will be used to be resized to side_size . device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will visualize the optical flow for each pair of RGB frames. Examples Please see the examples for RAFT optical flow frame extraction. Make sure to replace --feature_type argument to pwc . Credits The PWC-Net paper and official implementation . The PyTorch implementation used in this repo . License The wrapping code is under MIT, but PWC Net has GPL-3.0","title":"PWC-Net"},{"location":"models/pwc/#pwc-net","text":"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume frames are extracted for every consecutive pair of frames in a video. PWC-Net is pre-trained on Sintel Flow dataset . The implementation follows sniklaus/pytorch-pwc@f61389005 . CUDA 11 and GPUs like RTX 3090 and newer The current environment does not support CUDA 11 and, therefore, GPUs like RTX 3090 and newer. For details please check this issue #13 If you were able to fix it, please share your workaround. If you need an optical flow extractor, you are recommended to use RAFT . The PWC-Net does NOT support using CPU currently The PWC-Net uses cupy module, which makes it difficult to turn to a version that does not use the GPU. However, if you have solution, you may submit a PR.","title":"PWC-Net"},{"location":"models/pwc/#set-up-the-environment-for-pwc","text":"Setup conda environment. # it will create a new conda environment called 'pwc' on your machine conda env create -f conda_env_pwc.yml","title":"Set up the Environment for PWC"},{"location":"models/pwc/#quick-start","text":"Activate the environment conda activate pwc and extract optical flow from ./sample/v_GGSY1Qvo990.mp4 and show the flow for each frame python main.py \\ feature_type=pwc \\ device=\"cuda:0\" \\ show_pred=true \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" Note , if show_pred=true , the window with predictions will appear, use any key to show the next frame. To use show_pred=true , a screen must be attached to the machine or X11 forwarding is enabled.","title":"Quick Start"},{"location":"models/pwc/#supported-arguments","text":"Argument Default Description batch_size 1 You may speed up extraction of features by increasing the batch size as much as your GPU permits. extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. side_size null If resized to the smaller edge ( resize_to_smaller_edge=true ), then min(W, H) = side_size , if to the larger: max(W, H), if null (None) no resize is performed. resize_to_smaller_edge true If false , the larger edge will be used to be resized to side_size . device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will visualize the optical flow for each pair of RGB frames.","title":"Supported Arguments"},{"location":"models/pwc/#examples","text":"Please see the examples for RAFT optical flow frame extraction. Make sure to replace --feature_type argument to pwc .","title":"Examples"},{"location":"models/pwc/#credits","text":"The PWC-Net paper and official implementation . The PyTorch implementation used in this repo .","title":"Credits"},{"location":"models/pwc/#license","text":"The wrapping code is under MIT, but PWC Net has GPL-3.0","title":"License"},{"location":"models/r21d/","text":"R(2+1)D We support 3 flavors of R(2+1)D: r2plus1d_18_16_kinetics 18-layer R(2+1)D pre-trained on Kinetics 400 (used by default) \u2013 it is identical to the torchvision implementation r2plus1d_34_32_ig65m_ft_kinetics 34-layer R(2+1)D pre-trained on IG-65M and fine-tuned on Kinetics 400 \u2013 the weights are provided by moabitcoin/ig65m-pytorch repo for stack/step size 32 . r2plus1d_34_8_ig65m_ft_kinetics the same as the one above but this one was pre-trained with stack/step size 8 models are pre-trained on RGB frames and follow the plain torchvision augmentation sequence . Info The flavors that were pre-trained on IG-65M and fine-tuned on Kinetics 400 yield significantly better performance than the default model (e.g. the 32 frame model reaches an accuracy of 79.10 vs 57.50 by default). By default ( model_name=r2plus1d_18_16_kinetics ), the model expects to input a stack of 16 RGB frames ( 112x112 ), which spans 0.64 seconds of the video recorded at 25 fps. In the default case, the features will be of size Tv x 512 where Tv = duration / 0.64 . Specify, model_name , step_size and stack_size to change the default behavior. Set up the Environment for R(2+1)D Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml Quick Start Activate the environment conda activate torch_zoo and extract features from the ./sample/v_GGSY1Qvo990.mp4 video and show the predicted classes python main.py \\ feature_type=r21d \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ show_pred=true Supported Arguments Argument Default Description model_name \"r2plus1d_18_16_kinetics\" A variant of R(2+1)d. \"r2plus1d_18_16_kinetics\" , \"r2plus1d_34_32_ig65m_ft_kinetics\" , \"r2plus1d_34_8_ig65m_ft_kinetics\" are supported. stack_size null The number of frames from which to extract features (or window size). If omitted, it will respect the config of model_name during training. step_size null The number of frames to step before extracting the next features. If omitted, it will respect the config of model_name during training. extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will print the predictions of the model on a down-stream task. It is useful for debugging. Example Start by activating the environment conda activate torch_zoo It will extract R(2+1)d features for two sample videos. The features are going to be extracted with the default parameters. python main.py \\ feature_type=r21d \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Here is an example with r2plus1d_34_32_ig65m_ft_kinetics 34-layer R(2+1)D model that waas pre-trained on IG-65M and, then, fine-tuned on Kinetics 400 python main.py \\ feature_type=r21d \\ model_name=\"r2plus1d_34_8_ig65m_ft_kinetics\" \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" See the config file for other supported parameters. Note, that this implementation of R(2+1)d only supports the RGB stream. Credits The TorchVision implementation . The R(2+1)D paper: A Closer Look at Spatiotemporal Convolutions for Action Recognition . Thanks to @ohjho we now also support the favors of the 34-layer model pre-trained on IG-65M and fine-tuned on Kinetics 400 . A shout-out to devs of moabitcoin/ig65m-pytorch who adapted weights of these favors from Caffe to PyTorch. The paper where these flavors were presented: Large-scale weakly-supervised pre-training for video action recognition License The wrapping code is under MIT, yet, it utilizes torchvision library which is under BSD 3-Clause \"New\" or \"Revised\" License .","title":"R(2+1)D"},{"location":"models/r21d/#r21d","text":"We support 3 flavors of R(2+1)D: r2plus1d_18_16_kinetics 18-layer R(2+1)D pre-trained on Kinetics 400 (used by default) \u2013 it is identical to the torchvision implementation r2plus1d_34_32_ig65m_ft_kinetics 34-layer R(2+1)D pre-trained on IG-65M and fine-tuned on Kinetics 400 \u2013 the weights are provided by moabitcoin/ig65m-pytorch repo for stack/step size 32 . r2plus1d_34_8_ig65m_ft_kinetics the same as the one above but this one was pre-trained with stack/step size 8 models are pre-trained on RGB frames and follow the plain torchvision augmentation sequence . Info The flavors that were pre-trained on IG-65M and fine-tuned on Kinetics 400 yield significantly better performance than the default model (e.g. the 32 frame model reaches an accuracy of 79.10 vs 57.50 by default). By default ( model_name=r2plus1d_18_16_kinetics ), the model expects to input a stack of 16 RGB frames ( 112x112 ), which spans 0.64 seconds of the video recorded at 25 fps. In the default case, the features will be of size Tv x 512 where Tv = duration / 0.64 . Specify, model_name , step_size and stack_size to change the default behavior.","title":"R(2+1)D"},{"location":"models/r21d/#set-up-the-environment-for-r21d","text":"Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for R(2+1)D"},{"location":"models/r21d/#quick-start","text":"Activate the environment conda activate torch_zoo and extract features from the ./sample/v_GGSY1Qvo990.mp4 video and show the predicted classes python main.py \\ feature_type=r21d \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ show_pred=true","title":"Quick Start"},{"location":"models/r21d/#supported-arguments","text":"Argument Default Description model_name \"r2plus1d_18_16_kinetics\" A variant of R(2+1)d. \"r2plus1d_18_16_kinetics\" , \"r2plus1d_34_32_ig65m_ft_kinetics\" , \"r2plus1d_34_8_ig65m_ft_kinetics\" are supported. stack_size null The number of frames from which to extract features (or window size). If omitted, it will respect the config of model_name during training. step_size null The number of frames to step before extracting the next features. If omitted, it will respect the config of model_name during training. extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will print the predictions of the model on a down-stream task. It is useful for debugging.","title":"Supported Arguments"},{"location":"models/r21d/#example","text":"Start by activating the environment conda activate torch_zoo It will extract R(2+1)d features for two sample videos. The features are going to be extracted with the default parameters. python main.py \\ feature_type=r21d \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Here is an example with r2plus1d_34_32_ig65m_ft_kinetics 34-layer R(2+1)D model that waas pre-trained on IG-65M and, then, fine-tuned on Kinetics 400 python main.py \\ feature_type=r21d \\ model_name=\"r2plus1d_34_8_ig65m_ft_kinetics\" \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" See the config file for other supported parameters. Note, that this implementation of R(2+1)d only supports the RGB stream.","title":"Example"},{"location":"models/r21d/#credits","text":"The TorchVision implementation . The R(2+1)D paper: A Closer Look at Spatiotemporal Convolutions for Action Recognition . Thanks to @ohjho we now also support the favors of the 34-layer model pre-trained on IG-65M and fine-tuned on Kinetics 400 . A shout-out to devs of moabitcoin/ig65m-pytorch who adapted weights of these favors from Caffe to PyTorch. The paper where these flavors were presented: Large-scale weakly-supervised pre-training for video action recognition","title":"Credits"},{"location":"models/r21d/#license","text":"The wrapping code is under MIT, yet, it utilizes torchvision library which is under BSD 3-Clause \"New\" or \"Revised\" License .","title":"License"},{"location":"models/raft/","text":"RAFT Recurrent All-Pairs Field Transforms for Optical Flow (RAFT) frames are extracted for every consecutive pair of frames in a video. The implementation follows the official implementation . RAFT is pre-trained on FlyingChairs , fine-tuned on FlyingThings3D , then it is finetuned on Sintel or KITTI-2015 (see the Training Schedule in the Experiments section in the RAFT paper). Also, check out and this issue to learn more about the shared models. The optical flow frames have the same size as the video input or as specified by the resize arguments. We additionally output timesteps in ms for each feature and fps of the video. Set up the Environment for RAFT Setup conda environment. Requirements for RAFT are similar to the torchvision zoo, which uses conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml Quick Start Activate the environment conda activate torch_zoo and extract optical flow from ./sample/v_GGSY1Qvo990.mp4 using one GPU and show the flow for each frame python main.py \\ feature_type=raft \\ show_pred=true \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" Note , if show_pred=true , the window with predictions will appear, use any key to show the next frame. To use show_pred=true , a screen must be attached to the machine or X11 forwarding is enabled. Supported Arguments Argument Default Description finetuned_on sintel The RAFT model is pre-trained on FlyingChairs, then it is fine-tuned on FlyingThings3D, and then it is fine-tuned on finetuned_on dataset that can be either sintel or kitti . batch_size 1 You may speed up extraction of features by increasing the batch size as much as your GPU permits. extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. side_size null If resized to the smaller edge ( resize_to_smaller_edge=true ), then min(W, H) = side_size , if to the larger: max(W, H), if null (None) no resize is performed. resize_to_smaller_edge true If false , the larger edge will be used to be resized to side_size . device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will visualize the optical flow for each pair of RGB frames. Examples Start by activating the environment conda activate torch_zoo A minimal working example: it will extract RAFT optical flow frames for sample videos. python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Note, if your videos are quite long, have large dimensions and fps, watch your RAM as the frames are stored in the memory until they are saved. Please see other examples how can you overcome this problem. By default, the frames are extracted using the Sintel model. If you wish you can use KITTI-pretrained model by changing the finetuned_on argument: python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ finetuned_on=kitti \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to save the frames, use --on_extraction save_numpy (or save_pickle ) \u2013 by default, the frames are saved in ./output/ or where --output_path specifies. In the case of RAFT, besides frames, it also saves timestamps in ms and the original fps of the video into the same folder with features. python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ on_extraction=save_numpy \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Since extracting flow between two frames is cheap we may increase the extraction speed with batching. Therefore, you can use --batch_size argument (defaults to 1 ) to do so. A precaution: make sure to properly test the memory impact of using a specific batch size if you are not sure which kind of videos you have. For instance, you tested the extraction on 16:9 aspect ratio videos but some videos are 16:10 which might give you a mem error. Therefore, I would recommend to tune --batch_size on a square video and using the resize arguments (showed later) python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ batch_size=16 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Another way of speeding up the extraction is to resize the input frames. Use resize_to_smaller_edge=true (default) if you would like --side_size to be min(W, H) if resize_to_smaller_edge=false the --side_size value will correspond to be max(W, H) . The latter might be useful when you are not sure which aspect ratio the videos have (the upper bound on size). python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ side_size=256 \\ resize_to_smaller_edge=false \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If the videos have different fps rate, --extraction_fps might be used to specify the target fps of all videos (a video is reencoded and saved to --tmp_path folder and deleted if --keep_tmp_files wasn't used). python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ extraction_fps=1 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Credits The Official RAFT implementation (esp. ./demo.py ) . The RAFT paper: RAFT: Recurrent All Pairs Field Transforms for Optical Flow . License The wrapping code is under MIT, but the RAFT implementation complies with BSD 3-Clause .","title":"RAFT"},{"location":"models/raft/#raft","text":"Recurrent All-Pairs Field Transforms for Optical Flow (RAFT) frames are extracted for every consecutive pair of frames in a video. The implementation follows the official implementation . RAFT is pre-trained on FlyingChairs , fine-tuned on FlyingThings3D , then it is finetuned on Sintel or KITTI-2015 (see the Training Schedule in the Experiments section in the RAFT paper). Also, check out and this issue to learn more about the shared models. The optical flow frames have the same size as the video input or as specified by the resize arguments. We additionally output timesteps in ms for each feature and fps of the video.","title":"RAFT"},{"location":"models/raft/#set-up-the-environment-for-raft","text":"Setup conda environment. Requirements for RAFT are similar to the torchvision zoo, which uses conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for RAFT"},{"location":"models/raft/#quick-start","text":"Activate the environment conda activate torch_zoo and extract optical flow from ./sample/v_GGSY1Qvo990.mp4 using one GPU and show the flow for each frame python main.py \\ feature_type=raft \\ show_pred=true \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" Note , if show_pred=true , the window with predictions will appear, use any key to show the next frame. To use show_pred=true , a screen must be attached to the machine or X11 forwarding is enabled.","title":"Quick Start"},{"location":"models/raft/#supported-arguments","text":"Argument Default Description finetuned_on sintel The RAFT model is pre-trained on FlyingChairs, then it is fine-tuned on FlyingThings3D, and then it is fine-tuned on finetuned_on dataset that can be either sintel or kitti . batch_size 1 You may speed up extraction of features by increasing the batch size as much as your GPU permits. extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. side_size null If resized to the smaller edge ( resize_to_smaller_edge=true ), then min(W, H) = side_size , if to the larger: max(W, H), if null (None) no resize is performed. resize_to_smaller_edge true If false , the larger edge will be used to be resized to side_size . device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will visualize the optical flow for each pair of RGB frames.","title":"Supported Arguments"},{"location":"models/raft/#examples","text":"Start by activating the environment conda activate torch_zoo A minimal working example: it will extract RAFT optical flow frames for sample videos. python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Note, if your videos are quite long, have large dimensions and fps, watch your RAM as the frames are stored in the memory until they are saved. Please see other examples how can you overcome this problem. By default, the frames are extracted using the Sintel model. If you wish you can use KITTI-pretrained model by changing the finetuned_on argument: python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ finetuned_on=kitti \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to save the frames, use --on_extraction save_numpy (or save_pickle ) \u2013 by default, the frames are saved in ./output/ or where --output_path specifies. In the case of RAFT, besides frames, it also saves timestamps in ms and the original fps of the video into the same folder with features. python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ on_extraction=save_numpy \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Since extracting flow between two frames is cheap we may increase the extraction speed with batching. Therefore, you can use --batch_size argument (defaults to 1 ) to do so. A precaution: make sure to properly test the memory impact of using a specific batch size if you are not sure which kind of videos you have. For instance, you tested the extraction on 16:9 aspect ratio videos but some videos are 16:10 which might give you a mem error. Therefore, I would recommend to tune --batch_size on a square video and using the resize arguments (showed later) python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ batch_size=16 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Another way of speeding up the extraction is to resize the input frames. Use resize_to_smaller_edge=true (default) if you would like --side_size to be min(W, H) if resize_to_smaller_edge=false the --side_size value will correspond to be max(W, H) . The latter might be useful when you are not sure which aspect ratio the videos have (the upper bound on size). python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ side_size=256 \\ resize_to_smaller_edge=false \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If the videos have different fps rate, --extraction_fps might be used to specify the target fps of all videos (a video is reencoded and saved to --tmp_path folder and deleted if --keep_tmp_files wasn't used). python main.py \\ feature_type=raft \\ device=\"cuda:0\" \\ extraction_fps=1 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"","title":"Examples"},{"location":"models/raft/#credits","text":"The Official RAFT implementation (esp. ./demo.py ) . The RAFT paper: RAFT: Recurrent All Pairs Field Transforms for Optical Flow .","title":"Credits"},{"location":"models/raft/#license","text":"The wrapping code is under MIT, but the RAFT implementation complies with BSD 3-Clause .","title":"License"},{"location":"models/resnet/","text":"ResNet The ResNet features are extracted at each frame of the provided video. The ResNet is pre-trained on the 1k ImageNet dataset. We extract features from the pre-classification layer. The implementation is based on the torchvision models . The extracted features are going to be of size num_frames x 2048 . We additionally output timesteps in ms for each feature and fps of the video. We use the standard set of augmentations. Set up the Environment for ResNet Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml Quick Start Activate the environment conda activate torch_zoo and extract features at 1 fps from ./sample/v_GGSY1Qvo990.mp4 video and show the predicted classes python main.py \\ feature_type=resnet \\ model_name=resnet101 \\ extraction_fps=1 \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ show_pred=true Supported Arguments Argument Default Description model_name resnet50 A variant of ResNet. resnet18 resnet34 resnet50 resnet101 resnet151 are supported. batch_size 1 You may speed up extraction of features by increasing the batch size as much as your GPU permits. extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will print the predictions of the model on a down-stream task. It is useful for debugging. Examples Start by activating the environment conda activate torch_zoo It is pretty much the same procedure as with other features. The example is provided for the ResNet-50 flavour, but we also support ResNet-18,34,101,152. You can specify the model with the model_name parameter python main.py \\ feature_type=resnet \\ model_name=resnet50 \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to save the features, use --on_extraction save_numpy (or save_pickle ) \u2013 by default, the features are saved in ./output/ or where --output_path specifies. In the case of frame-wise features, besides features, it also saves timestamps in ms and the original fps of the video into the same folder with features. python main.py \\ feature_type=resnet \\ model_name=resnet50 \\ device=\"cuda:0\" \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt Since these features are so fine-grained and light-weight we may increase the extraction speed with batching. Therefore, frame-wise features have --batch_size argument, which defaults to 1 . python main.py \\ feature_type=resnet \\ model_name=resnet50 \\ device=\"cuda:0\" \\ batch_size=128 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to extract features at a certain fps, add --extraction_fps argument python main.py \\ feature_type=resnet \\ model_name=resnet50 \\ device=\"cuda:0\" \\ extraction_fps=5 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Credits The TorchVision implementation . The ResNet paper License The wrapping code is under MIT, yet, it utilizes torchvision library which is under BSD 3-Clause \"New\" or \"Revised\" License .","title":"ResNet"},{"location":"models/resnet/#resnet","text":"The ResNet features are extracted at each frame of the provided video. The ResNet is pre-trained on the 1k ImageNet dataset. We extract features from the pre-classification layer. The implementation is based on the torchvision models . The extracted features are going to be of size num_frames x 2048 . We additionally output timesteps in ms for each feature and fps of the video. We use the standard set of augmentations.","title":"ResNet"},{"location":"models/resnet/#set-up-the-environment-for-resnet","text":"Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for ResNet"},{"location":"models/resnet/#quick-start","text":"Activate the environment conda activate torch_zoo and extract features at 1 fps from ./sample/v_GGSY1Qvo990.mp4 video and show the predicted classes python main.py \\ feature_type=resnet \\ model_name=resnet101 \\ extraction_fps=1 \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ show_pred=true","title":"Quick Start"},{"location":"models/resnet/#supported-arguments","text":"Argument Default Description model_name resnet50 A variant of ResNet. resnet18 resnet34 resnet50 resnet101 resnet151 are supported. batch_size 1 You may speed up extraction of features by increasing the batch size as much as your GPU permits. extraction_fps null If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will print the predictions of the model on a down-stream task. It is useful for debugging.","title":"Supported Arguments"},{"location":"models/resnet/#examples","text":"Start by activating the environment conda activate torch_zoo It is pretty much the same procedure as with other features. The example is provided for the ResNet-50 flavour, but we also support ResNet-18,34,101,152. You can specify the model with the model_name parameter python main.py \\ feature_type=resnet \\ model_name=resnet50 \\ device=\"cuda:0\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to save the features, use --on_extraction save_numpy (or save_pickle ) \u2013 by default, the features are saved in ./output/ or where --output_path specifies. In the case of frame-wise features, besides features, it also saves timestamps in ms and the original fps of the video into the same folder with features. python main.py \\ feature_type=resnet \\ model_name=resnet50 \\ device=\"cuda:0\" \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt Since these features are so fine-grained and light-weight we may increase the extraction speed with batching. Therefore, frame-wise features have --batch_size argument, which defaults to 1 . python main.py \\ feature_type=resnet \\ model_name=resnet50 \\ device=\"cuda:0\" \\ batch_size=128 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to extract features at a certain fps, add --extraction_fps argument python main.py \\ feature_type=resnet \\ model_name=resnet50 \\ device=\"cuda:0\" \\ extraction_fps=5 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"","title":"Examples"},{"location":"models/resnet/#credits","text":"The TorchVision implementation . The ResNet paper","title":"Credits"},{"location":"models/resnet/#license","text":"The wrapping code is under MIT, yet, it utilizes torchvision library which is under BSD 3-Clause \"New\" or \"Revised\" License .","title":"License"},{"location":"models/s3d/","text":"S3D The S3D action recognition model was originally introduced in Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification . We support the PyTorch weights for Kinetics 400 provided by github.com/kylemin/S3D . According to the model card, with these weights, the model should achieve 72.08% top-1 accuracy (top5: 90.35%) on the Kinetics 400 validation set. How the model was pre-trained? My best educated guess is that the model was trained on densely sampled 64-frame 224 x 224 stacks that were randomly trimmed and cropped from 25 fps 256 x 256 video clips (<= 10 sec). Therefore, to extract features ( Tv x 1024 ), we resize the input video such that min(H, W) = 224 (?) and take the center crop to make it 224 x 224 . By default, the feature extractor will split the input video into 64-stack frames (2.56 sec) with no overlap as it is during the pre-training and will do a forward pass on each of them. This should be similar to I3D behavior. For instance, given an ~18-second 25 fps video, the features will be of size 7 x 1024 . Specify, step_size , extraction_fps , and stack_size to change the default behavior. What is extracted exactly? The inputs to the classification head (see S3D.fc and S3D.forward ) that were average-pooled across the time dimension. Set up the Environment for S3D Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml Quick Start Activate the environment conda activate torch_zoo and extract features from the ./sample/v_GGSY1Qvo990.mp4 video and show the predicted classes python main.py \\ feature_type=s3d \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ show_pred=true See the config file for ther supported parameters. Supported Arguments Argument Default Description stack_size 64 The number of frames from which to extract features (or window size). If omitted, it will respect the config of model_name during training. step_size 64 The number of frames to step before extracting the next features. If omitted, it will respect the config of model_name during training. extraction_fps 25 If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will print the predictions of the model on a down-stream task. It is useful for debugging. Credits The kylemin/S3D implementation. The S3D paper: Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification . License MIT","title":"S3D"},{"location":"models/s3d/#s3d","text":"The S3D action recognition model was originally introduced in Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification . We support the PyTorch weights for Kinetics 400 provided by github.com/kylemin/S3D . According to the model card, with these weights, the model should achieve 72.08% top-1 accuracy (top5: 90.35%) on the Kinetics 400 validation set. How the model was pre-trained? My best educated guess is that the model was trained on densely sampled 64-frame 224 x 224 stacks that were randomly trimmed and cropped from 25 fps 256 x 256 video clips (<= 10 sec). Therefore, to extract features ( Tv x 1024 ), we resize the input video such that min(H, W) = 224 (?) and take the center crop to make it 224 x 224 . By default, the feature extractor will split the input video into 64-stack frames (2.56 sec) with no overlap as it is during the pre-training and will do a forward pass on each of them. This should be similar to I3D behavior. For instance, given an ~18-second 25 fps video, the features will be of size 7 x 1024 . Specify, step_size , extraction_fps , and stack_size to change the default behavior. What is extracted exactly? The inputs to the classification head (see S3D.fc and S3D.forward ) that were average-pooled across the time dimension.","title":"S3D"},{"location":"models/s3d/#set-up-the-environment-for-s3d","text":"Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for S3D"},{"location":"models/s3d/#quick-start","text":"Activate the environment conda activate torch_zoo and extract features from the ./sample/v_GGSY1Qvo990.mp4 video and show the predicted classes python main.py \\ feature_type=s3d \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ show_pred=true See the config file for ther supported parameters.","title":"Quick Start"},{"location":"models/s3d/#supported-arguments","text":"Argument Default Description stack_size 64 The number of frames from which to extract features (or window size). If omitted, it will respect the config of model_name during training. step_size 64 The number of frames to step before extracting the next features. If omitted, it will respect the config of model_name during training. extraction_fps 25 If specified (e.g. as 5 ), the video will be re-encoded to the extraction_fps fps. Leave unspecified or null to skip re-encoding. device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). show_pred false If true , the script will print the predictions of the model on a down-stream task. It is useful for debugging.","title":"Supported Arguments"},{"location":"models/s3d/#credits","text":"The kylemin/S3D implementation. The S3D paper: Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification .","title":"Credits"},{"location":"models/s3d/#license","text":"MIT","title":"License"},{"location":"models/vggish/","text":"VGGish The VGGish feature extraction relies on the PyTorch implementation by harritaylor built to replicate the procedure provided in the TensorFlow repository . The difference in values between the PyTorch and Tensorflow implementation is negligible (see also # difference in values ). The VGGish model was pre-trained on AudioSet . The extracted features are from pre-classification layer after activation. The feature tensor will be 128-d and correspond to 0.96 sec of the original video. Interestingly, this might be represented as 24 frames of a 25 fps video. Therefore, you should expect Ta x 128 features, where Ta = duration / 0.96 . The extraction of VGGish features is implemeted as a wrapper of the TensorFlow implementation. See Credits . Set up the Environment for VGGish Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml Quick Start Activate the environment conda activate torch_zoo and extract features from the ./sample/v_GGSY1Qvo990.mp4 video python main.py \\ feature_type=vggish \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" Supported Arguments Argument Default Description device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos). Example The video paths can be specified as a .txt file with paths. python main.py \\ feature_type=vggish \\ device=\"cuda:0\" \\ file_with_video_paths=./sample/sample_video_paths.txt The features can be saved as numpy arrays by specifying --on_extraction save_numpy or save_pickle . By default, it will create a folder ./output and will store features there (you can change the output folder using --output_path ) python main.py \\ feature_type=vggish \\ device=\"cuda:0\" \\ on_extraction=save_numpy \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Difference between Tensorflow and PyTorch implementations python main.py \\ feature_type=vggish \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt TF (./sample/v_GGSY1Qvo990.mp4): [[0. 0.04247099 0.09079538 ... 0. 0.18485409 0. ] [0. 0. 0. ... 0. 0.5720243 0.5475726 ] [0. 0.00705254 0.15173683 ... 0. 0.33540994 0.10572422] ... [0. 0. 0.36020872 ... 0. 0.08559107 0.00870359] [0. 0.21485361 0.16507196 ... 0. 0. 0. ] [0. 0.31638345 0. ... 0. 0. 0. ]] max: 2.31246495; mean: 0.13741589; min: 0.00000000 PyTorch (./sample/v_GGSY1Qvo990.mp4): [[0. 0.04247095 0.09079528 ... 0. 0.18485469 0. ] [0. 0. 0. ... 0. 0.5720252 0.5475726 ] [0. 0.0070536 0.1517372 ... 0. 0.33541012 0.10572463] ... [0. 0. 0.36020786 ... 0. 0.08559084 0.00870359] [0. 0.21485506 0.16507116 ... 0. 0. 0. ] [0. 0.31638315 0. ... 0. 0. 0. ]] max: 2.31246495; mean: 0.13741589; min: 0.00000000 (PyTorch - TensorFlow).abs() tensor([[0.0000e+00, 4.4703e-08, 1.0431e-07, ..., 0.0000e+00, 5.9605e-07, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 8.9407e-07, 0.0000e+00], [0.0000e+00, 1.0580e-06, 3.7253e-07, ..., 0.0000e+00, 1.7881e-07, 4.1723e-07], ..., [0.0000e+00, 0.0000e+00, 8.6427e-07, ..., 0.0000e+00, 2.3097e-07, 0.0000e+00], [0.0000e+00, 1.4454e-06, 8.0466e-07, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 2.9802e-07, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]]) max: 4.0531e-06; mean: 2.2185e-07; min: 0.00000000 Credits The PyTorch implementation of vggish . The VGGish paper: CNN Architectures for Large-Scale Audio Classification . License The wrapping code is under MIT but the vggish implementation complies with the harritaylor/torchvggish (same as tensorflow) license which is Apache-2.0 .","title":"VGGish"},{"location":"models/vggish/#vggish","text":"The VGGish feature extraction relies on the PyTorch implementation by harritaylor built to replicate the procedure provided in the TensorFlow repository . The difference in values between the PyTorch and Tensorflow implementation is negligible (see also # difference in values ). The VGGish model was pre-trained on AudioSet . The extracted features are from pre-classification layer after activation. The feature tensor will be 128-d and correspond to 0.96 sec of the original video. Interestingly, this might be represented as 24 frames of a 25 fps video. Therefore, you should expect Ta x 128 features, where Ta = duration / 0.96 . The extraction of VGGish features is implemeted as a wrapper of the TensorFlow implementation. See Credits .","title":"VGGish"},{"location":"models/vggish/#set-up-the-environment-for-vggish","text":"Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for VGGish"},{"location":"models/vggish/#quick-start","text":"Activate the environment conda activate torch_zoo and extract features from the ./sample/v_GGSY1Qvo990.mp4 video python main.py \\ feature_type=vggish \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\"","title":"Quick Start"},{"location":"models/vggish/#supported-arguments","text":"Argument Default Description device \"cuda:0\" The device specification. It follows the PyTorch style. Use \"cuda:3\" for the 4th GPU on the machine or \"cpu\" for CPU-only. video_paths null A list of videos for feature extraction. E.g. \"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" or just one path \"./sample/v_GGSY1Qvo990.mp4\" . file_with_video_paths null A path to a text file with video paths (one path per line). Hint: given a folder ./dataset with .mp4 files one could use: find ./dataset -name \"*mp4\" > ./video_paths.txt . on_extraction print If print , the features are printed to the terminal. If save_numpy or save_pickle , the features are saved to either .npy file or .pkl . output_path \"./output\" A path to a folder for storing the extracted features (if on_extraction is either save_numpy or save_pickle ). keep_tmp_files false If true , the reencoded videos will be kept in tmp_path . tmp_path \"./tmp\" A path to a folder for storing temporal files (e.g. reencoded videos).","title":"Supported Arguments"},{"location":"models/vggish/#example","text":"The video paths can be specified as a .txt file with paths. python main.py \\ feature_type=vggish \\ device=\"cuda:0\" \\ file_with_video_paths=./sample/sample_video_paths.txt The features can be saved as numpy arrays by specifying --on_extraction save_numpy or save_pickle . By default, it will create a folder ./output and will store features there (you can change the output folder using --output_path ) python main.py \\ feature_type=vggish \\ device=\"cuda:0\" \\ on_extraction=save_numpy \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"","title":"Example"},{"location":"models/vggish/#difference-between-tensorflow-and-pytorch-implementations","text":"python main.py \\ feature_type=vggish \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt TF (./sample/v_GGSY1Qvo990.mp4): [[0. 0.04247099 0.09079538 ... 0. 0.18485409 0. ] [0. 0. 0. ... 0. 0.5720243 0.5475726 ] [0. 0.00705254 0.15173683 ... 0. 0.33540994 0.10572422] ... [0. 0. 0.36020872 ... 0. 0.08559107 0.00870359] [0. 0.21485361 0.16507196 ... 0. 0. 0. ] [0. 0.31638345 0. ... 0. 0. 0. ]] max: 2.31246495; mean: 0.13741589; min: 0.00000000 PyTorch (./sample/v_GGSY1Qvo990.mp4): [[0. 0.04247095 0.09079528 ... 0. 0.18485469 0. ] [0. 0. 0. ... 0. 0.5720252 0.5475726 ] [0. 0.0070536 0.1517372 ... 0. 0.33541012 0.10572463] ... [0. 0. 0.36020786 ... 0. 0.08559084 0.00870359] [0. 0.21485506 0.16507116 ... 0. 0. 0. ] [0. 0.31638315 0. ... 0. 0. 0. ]] max: 2.31246495; mean: 0.13741589; min: 0.00000000 (PyTorch - TensorFlow).abs() tensor([[0.0000e+00, 4.4703e-08, 1.0431e-07, ..., 0.0000e+00, 5.9605e-07, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 8.9407e-07, 0.0000e+00], [0.0000e+00, 1.0580e-06, 3.7253e-07, ..., 0.0000e+00, 1.7881e-07, 4.1723e-07], ..., [0.0000e+00, 0.0000e+00, 8.6427e-07, ..., 0.0000e+00, 2.3097e-07, 0.0000e+00], [0.0000e+00, 1.4454e-06, 8.0466e-07, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 2.9802e-07, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]]) max: 4.0531e-06; mean: 2.2185e-07; min: 0.00000000","title":"Difference between Tensorflow and PyTorch implementations"},{"location":"models/vggish/#credits","text":"The PyTorch implementation of vggish . The VGGish paper: CNN Architectures for Large-Scale Audio Classification .","title":"Credits"},{"location":"models/vggish/#license","text":"The wrapping code is under MIT but the vggish implementation complies with the harritaylor/torchvggish (same as tensorflow) license which is Apache-2.0 .","title":"License"}]}